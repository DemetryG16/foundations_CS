{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f89049-7423-4b2d-a955-e9fc518d6765",
   "metadata": {},
   "source": [
    "# Foundations_CS EXAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3299958-0d07-4dfe-ade0-724e50c40874",
   "metadata": {},
   "source": [
    "How did I solve these exercises with the help of AI?\n",
    "* First, I do it using my knowldge from previous experience.\n",
    "* If I don't have previous experience, I search directly on google using key words (like in the first exercise 'concatenate') choosing the best way to apply.\n",
    "* I write the code my self. In case I couldn't solve errors, I use AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14605fd2-673c-4803-a312-d0cf307e8fe3",
   "metadata": {},
   "source": [
    "# 2025-26 Project\n",
    "You have to work on the Trending YouTube dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6224ad56-7235-473c-add8-7af450a297d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4588e778-1436-4643-8c21-58aa5eaf27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de = pd.read_csv('https://raw.githubusercontent.com/DemetryG16/foundations_CS/main/trendingYT/DEvideos.csv')  \n",
    "                   # https://github.com/DemetryG16/foundations_CS.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b85758-5fe0-4fa1-9137-aa60b863fa95",
   "metadata": {},
   "source": [
    "### I will use the paths from my laptop as I can modify files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eecb5627-5118-486d-b6c3-341b93fdfd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LgVi6y5QIjM</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Sing zu Ende! | Gesangseinlagen vom Feinsten |...</td>\n",
       "      <td>inscope21</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T17:08:49.000Z</td>\n",
       "      <td>inscope21|\"sing zu ende\"|\"gesangseinlagen\"|\"ge...</td>\n",
       "      <td>252786</td>\n",
       "      <td>35885</td>\n",
       "      <td>230</td>\n",
       "      <td>1539</td>\n",
       "      <td>https://i.ytimg.com/vi/LgVi6y5QIjM/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Heute gibt es mal wieder ein neues Format... w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  LgVi6y5QIjM      17.14.11   \n",
       "\n",
       "                                               title channel_title  \\\n",
       "0  Sing zu Ende! | Gesangseinlagen vom Feinsten |...     inscope21   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           24  2017-11-13T17:08:49.000Z   \n",
       "\n",
       "                                                tags   views  likes  dislikes  \\\n",
       "0  inscope21|\"sing zu ende\"|\"gesangseinlagen\"|\"ge...  252786  35885       230   \n",
       "\n",
       "   comment_count                                  thumbnail_link  \\\n",
       "0           1539  https://i.ytimg.com/vi/LgVi6y5QIjM/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "\n",
       "                                         description  \n",
       "0  Heute gibt es mal wieder ein neues Format... w...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'I extracted all the files as it won't accept to load zipped files. Also, using raw zipped files might harm the memory'\n",
    "\n",
    "de = pd.read_csv('trendingYT/DEvideos.csv/DEvideos.csv')\n",
    "de.head(1)\n",
    "# listdir('trendingYT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7b3b7-6f56-4c83-b346-77d22120b5af",
   "metadata": {},
   "source": [
    "1. Create a single dataframe with the concatenation of all input csv files, adding a column called `country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23bb2427-31c9-4010-ae7e-2fa439fd6edf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrendingYT/*.csv/*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m all_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(path)\n\u001b[1;32m----> 4\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(all_files)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1895\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1894\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'list'>"
     ]
    }
   ],
   "source": [
    "# Previously, I know that I can use glob. But not with pandas tables\n",
    "path = 'trendingYT/*.csv/*.csv'\n",
    "all_files = glob.glob(path)\n",
    "list_all = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5e65c-d86e-4da8-a021-f2e131426fd8",
   "metadata": {},
   "source": [
    "2. Extract all videos that have no tag.\n",
    "3. For each channel, determine the total number of views\n",
    "4. Save all rows with disabled comments and disabled ratings, or that have `video_error_or_removed` in a new dataframe called `excluded`, and remove those rows from the original dataframe.\n",
    "5. Add a `like_ratio` column storing the ratio between the number of likes and of dislikes\n",
    "6. Cluster the publish time into 10-minute intervals (e.g. from 02:20 to 02:30)\n",
    "7. For each interval, determine the number of videos, average number of likes and of dislikes.\n",
    "8. For each tag, determine the number of videos\n",
    "Notice that `tags` contains a string with several tags.\n",
    "\n",
    "9. Find the tags with the largest number of videos\n",
    "10. For each (`tag`, `country`) pair, compute average ratio likes/dislikes\n",
    "11. For each (`trending_date`, `country`) pair, the video with the largest number of views\n",
    "12. Divide `trending_date` into three columns: `year`, `month`, `day`\n",
    "13. For each (`month`, `country`) pair, the video with the largest number of views\n",
    "14. Read all json files with the video categories\n",
    "15. For each country, determine how many videos have a category that is not assignable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb6d43-7bd6-4000-ab64-32bf78d5a864",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* It is mandatory to use GitHub for developing the project.\n",
    "* The project must be a jupyter notebook.\n",
    "* There is no restriction on the libraries that can be used, nor on the Python version.\n",
    "* All questions on the project must be asked in the Discussion forum on the course website.\n",
    "* At most 3 students can be in each group. You must create the groups by yourself. You can use the Discussion forum to create the groups.\n",
    "* You do not have to send me the project before the discussion.\n",
    "* You do not have to prepare any slides for the discussion.\n",
    "* You can use AI tools, but you have to describe in the notebook how you have used such tools and you have to show that you have fully understood everything that you have in your project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
